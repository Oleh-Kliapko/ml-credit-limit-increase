# train_model_base.py
# –ë–∞–∑–æ–≤–∏–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –±—É–¥—å-—è–∫–æ—ó –º–æ–¥–µ–ª—ñ
# –í–∫–ª—é—á–∞—î —Å–ø—ñ–ª—å–Ω—É –ª–æ–≥—ñ–∫—É: —Ä–æ–∑–ø–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö, —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è, –º–µ—Ç—Ä–∏–∫–∏, –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è, –≥—Ä–∞—Ñ—ñ–∫–∏

import os
import joblib
import logging
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score,
    fbeta_score,
    roc_curve,
    precision_recall_curve,
    auc
)

from .metrics.threshold_optimization import find_best_threshold
from .config import PROCESSED_DATA_PATH
from .metrics_report import build_metrics_report, final_conclusion, next_step_recommendation
from .metrics.precision_at_fixed_recall import (
    precision_at_fixed_recall,
    precision_recall_comment
)


logger = logging.getLogger(__name__)


def train_model_base(X, y, model, model_name, test_size=0.3, random_state=42):
    logger.info("\n=== –ü–æ–±—É–¥–æ–≤–∞ –º–æ–¥–µ–ª—ñ: %s ===", model_name)
    logger.info("–§—ñ—á: %s", list(X.columns))
    logger.info("Shape X: %s, y: %s", X.shape, y.shape)

    # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=test_size,
        random_state=random_state,
        stratify=y
    )

    logger.info("Train: %s, Test: %s", X_train.shape, X_test.shape)

    # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    model.fit(X_train, y_train)

    # –ü—Ä–æ–≥–Ω–æ–∑–∏
    y_proba = model.predict_proba(X_test)[:, 1]

    # –ü–æ—à—É–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥—É –¥–ª—è –±—ñ–Ω–∞—Ä–∏–∑–∞—Ü—ñ—ó –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π –∫–ª–∞—Å—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –º–µ—Ç—Ä–∏–∫–∏ F2
    best_threshold, best_f2 = find_best_threshold(y_test, y_proba, beta=2)
    logger.info(
        "üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –ø–æ—Ä—ñ–≥ = %.2f (max F2 = %.4f)",
        best_threshold, best_f2
    )
    y_pred_opt = (y_proba >= best_threshold).astype(int)

    # Precision –ø—Ä–∏ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–æ–º—É Recall = 0.8
    # –ë—ñ–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∞ "–Ø–∫—â–æ –º–∏ —Ö–æ—á–µ–º–æ –∑–ª–æ–≤–∏—Ç–∏ –Ω–µ –º–µ–Ω—à–µ 80% —Ä–∏–∑–∏–∫–æ–≤–∏—Ö –∫–ª—ñ—î–Ω—Ç—ñ–≤,
    # —Ç–æ –Ω–∞—Å–∫—ñ–ª—å–∫–∏ —á–∏—Å—Ç–∏–º –±—É–¥–µ —Å–ø–∏—Å–æ–∫, —è–∫–∏–π –º–∏ –ø–µ—Ä–µ–¥–∞–º–æ –≤ —Ä–æ–±–æ—Ç—É?"
    result = precision_at_fixed_recall(
        y_true=y_test,
        y_proba=y_proba,
        target_recall=0.8
    )
    comment = precision_recall_comment(result["precision"])
    print("\n=== Precision @ Fixed Recall ===\n")
    print("Target recall:         0.80")
    print(f"Optimal threshold:    {result['threshold']:.3f}")
    print(f"Recall achieved:      {result['recall']:.3f}")
    print(f"Precision achieved:   {result['precision']:.3f}")
    print(f"Conclusion:           {comment}")

    # –ú–µ—Ç—Ä–∏–∫–∏
    auc_metric = roc_auc_score(y_test, y_proba)
    gini = 2 * auc_metric - 1
    precision = precision_score(y_test, y_pred_opt)
    recall = recall_score(y_test, y_pred_opt)
    f1 = f1_score(y_test, y_pred_opt)
    f2 = fbeta_score(y_test, y_pred_opt, beta=2)

    metrics = {
        "auc": auc_metric,
        "gini": gini,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "f2": f2,
    }

    build_metrics_report(metrics)

    print(f"\n=== üß† –§—ñ–Ω–∞–ª—å–Ω–∏–π –≤–∏—Å–Ω–æ–≤–æ–∫ ({model_name}) ===")
    print(final_conclusion(metrics))

    print(f"\n=== üõ† –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –Ω–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏ ({model_name}) ===")
    print(next_step_recommendation(metrics))

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    model_path = os.path.join(PROCESSED_DATA_PATH, f"{model_name}.pkl")
    joblib.dump(model, model_path)
    logger.info("\n–ú–æ–¥–µ–ª—å %s –∑–±–µ—Ä–µ–∂–µ–Ω–∞: %s", model_name, model_path)

    # –ì—Ä–∞—Ñ—ñ–∫–∏ ROC + PR
    _, axes = plt.subplots(1, 2, figsize=(14, 6))

    # ROC –∫—Ä–∏–≤–∞
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    axes[0].plot(fpr, tpr, color='darkorange', lw=2,
                 label=f'ROC curve (AUC = {roc_auc:.3f})')
    axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    axes[0].set_xlim([0.0, 1.0])
    axes[0].set_ylim([0.0, 1.05])
    axes[0].set_xlabel('False Positive Rate')
    axes[0].set_ylabel('True Positive Rate')
    axes[0].set_title(f'ROC Curve - {model_name}')
    axes[0].legend(loc="lower right")
    axes[0].grid(True)

    # Precision-Recall –∫—Ä–∏–≤–∞
    pr_precision, pr_recall, _ = precision_recall_curve(y_test, y_proba)
    pr_auc = auc(pr_recall, pr_precision)
    axes[1].plot(pr_recall, pr_precision, color='green', lw=2,
                 label=f'PR curve (AUC = {pr_auc:.3f})')
    axes[1].set_xlabel('Recall')
    axes[1].set_ylabel('Precision')
    axes[1].set_title(f'Precision-Recall Curve - {model_name}')
    axes[1].legend(loc="upper right")
    axes[1].grid(True)

    plt.tight_layout()

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
    plots_path = os.path.join(
        PROCESSED_DATA_PATH, f"{model_name}_baseline_curves.png")
    plt.savefig(plots_path)
    logger.info("\n–ì—Ä–∞—Ñ—ñ–∫–∏ %s –∑–±–µ—Ä–µ–∂–µ–Ω–æ: %s", model_name, plots_path)
    plt.close()

    return {
        "model": model_name,
        "auc": auc_metric,
        "gini": gini,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "f2": f2,
        "opt_threshold": best_threshold,
        "precision_at_recall": result["precision"],
    }
